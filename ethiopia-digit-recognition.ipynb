{
 "cells": [
  {
   "source": [
    "# Get data stored in Google drive\n",
    "Don't run these 2 lines if one loads data from computer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xufW7f3KfpPa",
    "outputId": "0d23a6e8-e81b-4c6e-9e36-a1c41e994f79"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "source": [
    "# Import neccesary modules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6dKywt0fwlg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image  \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv2D, Dropout, MaxPool2D, BatchNormalization\n",
    "from tensorflow.keras import optimizers\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy"
   ]
  },
  {
   "source": [
    "# Load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axhwzEvxgvAs"
   },
   "outputs": [],
   "source": [
    "def load_training_image(dir):\n",
    "    \"\"\"\n",
    "    Load training data in image format (and save in it .npy format for later use)\n",
    "    \n",
    "    :param dir: Directory storing data\n",
    "    :return X_shuffle, y_shuffle: Shuffled training data and theirs one-hot encoded labels\n",
    "    \"\"\"\n",
    "\n",
    "    LABEL_LIST = [10, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    \n",
    "    X = []\n",
    "    y_onehot = np.zeros((60000, 10))\n",
    "    \n",
    "    for num in range(10):\n",
    "        sub_dir = os.path.join(dir, str(LABEL_LIST[num]))\n",
    "        for img_dir in os.listdir(sub_dir):\n",
    "            img = Image.open(os.path.join(sub_dir, img_dir))\n",
    "            array_img = np.array(img)\n",
    "            X.append(array_img)\n",
    "        \n",
    "        y_onehot[6000*(num):6000*(num+1), num] = 1  # Store label 10 as label 0\n",
    "    \n",
    "        # Show example of number\n",
    "        # img.show(title=str(num+1))\n",
    "        # plt.figure()\n",
    "        # plt.title(str(LABEL_LIST[num]))\n",
    "        # plt.imshow(array_img)\n",
    "        # plt.show()\n",
    "    \n",
    "    X = np.reshape(np.array(X), (60000, 28, 28, 1))\n",
    "    X_shuffle, y_shuffle = shuffle(X, y_onehot, random_state=1)\n",
    "    \n",
    "    # Save data and theirs label to .npy format\n",
    "    np.save('data.npy', X_shuffle)\n",
    "    np.save('label.npy', y_shuffle)\n",
    "    \n",
    "    return X_shuffle, y_shuffle\n",
    "\n",
    "def load_test_image(dir):\n",
    "    \"\"\"\n",
    "    Load test data in image format (and save in it .npy format for later use)\n",
    "    \n",
    "    :param dir: Directory storing data\n",
    "    :return X: Test data \n",
    "    \"\"\"\n",
    "\n",
    "    X = []\n",
    "\n",
    "    for img_dir in os.listdir(dir):\n",
    "        img = Image.open(os.path.join(dir, img_dir))\n",
    "        X.append(np.array(img))\n",
    "\n",
    "    X = np.reshape(np.array(X), (len(X), 28, 28, 1))\n",
    "    np.save(\"test.npy\", X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmJKpK7sjetI",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_folder = \"/content/drive/My Drive/Colab Notebooks/training_data\" \n",
    "X, y = load_training_image(my_folder)\n",
    "\n",
    "test_folder = \"/content/drive/My Drive/Colab Notebooks/test\"\n",
    "X_test = load_test_image(test_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = np.load(\"data.npy\"), np.load(\"data.npy\")\n",
    "X_test = np.load(\"test.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fYGAtnCEPS_"
   },
   "source": [
    "# Splitting dataset\n",
    "One can skip this part parameter \"validation_split\" is used in method fit() of tensorflow.keras.models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "odIa7tBvEPTA"
   },
   "outputs": [],
   "source": [
    "X_train = X[:50000, :, :] \n",
    "y_train = y[:50000, :] \n",
    "\n",
    "X_small = X_train[:1000, :, :]\n",
    "y_small = y[:1000, :] \n",
    "\n",
    "X_test = X[50000:, :, :] \n",
    "y_test = y[50000:, :] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvveXHDtEPTR"
   },
   "source": [
    "# Create model: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQxbMLgWEPTR"
   },
   "outputs": [],
   "source": [
    "model_cnn = Sequential([\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPool2D(pool_size=(2, 2)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPool2D(pool_size=(2, 2)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    Conv2D(256, (3, 3), activation='relu'),\n",
    "    MaxPool2D(pool_size=(2, 2)),\n",
    "    BatchNormalization(),\n",
    "        \n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(10, activation='softmax', activity_regularizer=l2(1e-4))\n",
    "    ])\n",
    "\n",
    "opt = optimizers.Adam(learning_rate=1e-3)\n",
    "model_cnn.compile(optimizer=opt, loss=CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "model_cnn.summary()\n"
   ]
  },
  {
   "source": [
    "# Fit model with training data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RNm1-qV74vxD",
    "outputId": "f34fb79f-6ef5-45ad-ca1c-c7ee87e1eeac"
   },
   "outputs": [],
   "source": [
    "history = model_cnn.fit(X, y, validation_split=0.2, epochs=200, verbose=1)"
   ]
  },
  {
   "source": [
    "# Plot accuracy and loss"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax[0].plot(history.history['accuracy'], label=\"Training accuracy\")\n",
    "ax[0].plot(history.history['val_accuracy'], label=\"Validation accuracy\")\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].set_ylim((0.99, 1.0))\n",
    "ax[0].legend(loc='lower right')\n",
    "ax[0].grid(\"on\")\n",
    "\n",
    "ax[1].plot(history.history['loss'], label=\"Training loss\")\n",
    "ax[1].plot(history.history['val_loss'], label=\"Validation loss\")\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_ylim((0.0, 0.05))\n",
    "ax[1].legend(loc='upper right')\n",
    "ax[1].grid(\"on\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model_cnn.predict(X_test)\n",
    "y_eval = np.argmax(y_prob, axis=1)\n",
    "\n",
    "# Change label 0 to 10\n",
    "y_eval[y_eval == 0] = 10\n",
    "\n",
    "# Save result to .csv file\n",
    "with open(\"sample_submission.csv\", \"w\") as fp: \n",
    "    fp.write(\"Id,Category\\n\") \n",
    "    for idx in range(10000): \n",
    "        fp.write(f\"{idx:05},{y_eval[idx]}\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvEhyt0CVmnm"
   },
   "source": [
    "# Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.save(\"cnn_mode_l.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ULSj0teVym-"
   },
   "source": [
    "# Loading model to run on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SgCVjP6gV2NU"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "new_model = load_model(\"cnn_mode_l.h5\")    # Can only save architechture, need to find a better way to save weight\n",
    "new_model.summary()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Copy of assignment1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('tensorflow_env': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dff50bd7c0266204f2691905f1787355563e9150473265e0cd715cae7ceee256"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}